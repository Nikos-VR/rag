import streamlit as st
import io
from langchain_core.messages import HumanMessage, AIMessage
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from PyPDF2 import PdfReader

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î¿Ï… Gemini Pro Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.5, google_api_key=st.secrets["GOOGLE_API_KEY"])

def get_document_chunks(pdf_docs):
    """ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® PDF ÏƒÎµ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î±Ï€ÏŒ Ï„Î¿ UploadedFile Î±Î½Ï„Î¹ÎºÎµÎ¯Î¼ÎµÎ½Î¿."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    
    all_text = ""
    for pdf_file in pdf_docs:
        # Î”Î¹Î¬Î²Î±ÏƒÎ¼Î± Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… ÏƒÏ„Î· Î¼Î½Î®Î¼Î·
        pdf_reader = PdfReader(io.BytesIO(pdf_file.getvalue()))
        for page in pdf_reader.pages:
            all_text += page.extract_text()
    
    return text_splitter.split_text(all_text)

def create_vector_store(text_chunks):
    """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î²Î¬ÏƒÎ·Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…."""
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_store = Chroma.from_texts(text_chunks, embeddings)
    return vector_store

def create_qa_chain(vector_store):
    """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î·Ï‚ Î±Î»Ï…ÏƒÎ¯Î´Î±Ï‚ (chain) Î³Î¹Î± ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚-Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚."""
    retriever = vector_store.as_retriever()
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î¿Ï… Streamlit UI
st.set_page_config(page_title="PDF Chatbot", layout="wide")
st.header("ğŸ’¬ PDF Chatbot Î¼Îµ Gemini Pro")

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚
if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# Î¦ÏŒÏÎ¼Î± Î³Î¹Î± Î±Î½Î­Î²Î±ÏƒÎ¼Î± Î±ÏÏ‡ÎµÎ¯Ï‰Î½
with st.sidebar:
    st.header("Î¦ÏŒÏÏ„Ï‰ÏƒÎµ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î¬ ÏƒÎ¿Ï…")
    uploaded_files = st.file_uploader(
        "Î‘Î½Î­Î²Î±ÏƒÎµ PDF Î±ÏÏ‡ÎµÎ¯Î±", type=["pdf"], accept_multiple_files=True
    )
    if uploaded_files:
        if st.button("Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±"):
            with st.spinner("Î•Ï€ÎµÎ¾ÎµÏÎ³Î¬Î¶Î¿Î¼Î±Î¹ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î±..."):
                text_chunks = get_document_chunks(uploaded_files)
                vector_store = create_vector_store(text_chunks)
                st.session_state.qa_chain = create_qa_chain(vector_store)
                st.success("Î¤Î± Î­Î³Î³ÏÎ±Ï†Î± Î­Ï‡Î¿Ï…Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„ÎµÎ¯ Î¼Îµ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±!")

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î¿Ï… Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Î•Î¯ÏƒÎ¿Î´Î¿Ï‚ Ï‡ÏÎ®ÏƒÏ„Î·
if prompt := st.chat_input("Î¡ÏÏ„Î·ÏƒÎ­ Î¼Îµ ÎºÎ¬Ï„Î¹ Î³Î¹Î± Ï„Î± Î­Î³Î³ÏÎ±Ï†Î± Ï€Î¿Ï… Î±Î½Î­Î²Î±ÏƒÎµÏ‚..."):
    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÎµÏÏÏ„Î·ÏƒÎ·Ï‚ Ï‡ÏÎ®ÏƒÏ„Î·
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Î Î±ÏÎ±Î³Ï‰Î³Î® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î±Ï€ÏŒ Ï„Î¿ chatbot
    with st.chat_message("assistant"):
        if st.session_state.qa_chain:
            # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÏƒÎµ ÏƒÏ…Î¼Î²Î±Ï„Î® Î¼Î¿ÏÏ†Î®
            chat_history_formatted = []
            for msg in st.session_state.messages:
                if msg["role"] == "user":
                    chat_history_formatted.append(HumanMessage(content=msg["content"]))
                else:
                    chat_history_formatted.append(AIMessage(content=msg["content"]))

            response = st.session_state.qa_chain({"question": prompt, "chat_history": chat_history_formatted})
            answer = response["answer"]
            st.markdown(answer)
        else:
            answer = "Î Î±ÏÎ±ÎºÎ±Î»Ï Ï†ÏŒÏÏ„Ï‰ÏƒÎµ ÎºÎ±Î¹ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î¬ÏƒÎ¿Ï… ÎºÎ¬Ï€Î¿Î¹Î± PDF Î±ÏÏ‡ÎµÎ¯Î± Î³Î¹Î± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÎ¹Ï‚."
            st.markdown(answer)
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ ÏƒÏ„Î¿ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ
    st.session_state.messages.append({"role": "assistant", "content": answer})